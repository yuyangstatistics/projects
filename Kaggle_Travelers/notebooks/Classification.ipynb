{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is about finding the optimal threshold with cost ratio being 5:1. 5 for miss fraud to make classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import model_selection\n",
    "from skopt import BayesSearchCV\n",
    "from matplotlib import pyplot\n",
    "from xgboost import plot_importance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, VotingClassifier)\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "from random import sample\n",
    "import random\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform dataset for XGBoost and LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####  Import Data  #####\n",
    "train = pd.read_csv(\"../data/train_data_clean_4_grouped.csv\")\n",
    "test = pd.read_csv(\"../data/test_data_clean_4_grouped.csv\")\n",
    "\n",
    "#####  Encode gender and living status and state  #####\n",
    "train[\"living_status\"] = pd.Categorical(train[\"living_status\"])\n",
    "train[\"gender\"] = np.where(train[\"gender\"].str.contains(\"M\"), 1, 0)\n",
    "train[\"living_status\"] = np.where(train[\"living_status\"].str.contains(\"Rent\"), 1, 0)\n",
    "\n",
    "test[\"living_status\"] = pd.Categorical(test[\"living_status\"])\n",
    "test[\"gender\"] = np.where(test[\"gender\"].str.contains(\"M\"), 1, 0)\n",
    "test[\"living_status\"] = np.where(test[\"living_status\"].str.contains(\"Rent\"), 1, 0)\n",
    "\n",
    "# one-hot encoding for site of state\n",
    "state_dummies = pd.get_dummies(test['state'], \n",
    "                                  prefix='state', drop_first=True)\n",
    "test = pd.concat([test, state_dummies], axis=1)\n",
    "test.drop([\"state\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for site of state\n",
    "state_dummies = pd.get_dummies(train['state'], \n",
    "                                  prefix='state', drop_first=True)\n",
    "train = pd.concat([train, state_dummies], axis=1)\n",
    "train.drop([\"state\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "##### Drop month, day and year data, drop vehicle color, zipcode, claim_date, claim_number and SP_Index  #####\n",
    "train.drop([\"claim_month_january\", \"claim_month_february\", \"claim_month_march\", \"claim_month_may\", \n",
    "              \"claim_month_june\", \"claim_month_july\", \"claim_month_august\", \"claim_month_september\", \n",
    "              \"claim_month_october\", \"claim_month_november\", \"claim_month_december\", \n",
    "              \"claim_day_monday\", \"claim_day_tuesday\", \"claim_day_wednesday\", \"claim_day_thursday\", \n",
    "               \"claim_day_saturday\", \"claim_day_sunday\", \"claim_year\", \"claim_day\", \n",
    "              \"zip_code\", \"claim_date\", \"claim_number\", 'SP_Index', \"vehicle_color_blue\", \n",
    "               \"vehicle_color_gray\", \"vehicle_color_other\", \"vehicle_color_red\", \n",
    "              \"vehicle_color_silver\", \"vehicle_color_white\"], axis =1, inplace=True)\n",
    "\n",
    "test.drop([\"claim_month_january\", \"claim_month_february\", \"claim_month_march\", \"claim_month_may\", \n",
    "              \"claim_month_june\", \"claim_month_july\", \"claim_month_august\", \"claim_month_september\", \n",
    "              \"claim_month_october\", \"claim_month_november\", \"claim_month_december\", \n",
    "              \"claim_day_monday\", \"claim_day_tuesday\", \"claim_day_wednesday\", \"claim_day_thursday\", \n",
    "               \"claim_day_saturday\", \"claim_day_sunday\", \"claim_year\", \"claim_day\", \n",
    "              \"zip_code\", \"claim_date\", \"claim_number\", 'SP_Index', \"vehicle_color_blue\", \n",
    "               \"vehicle_color_gray\", \"vehicle_color_other\", \"vehicle_color_red\", \n",
    "              \"vehicle_color_silver\", \"vehicle_color_white\"], axis =1, inplace=True)\n",
    "\n",
    "\n",
    "#####  Add saftyrating/(number of past claim) feature  #####\n",
    "train['per_saftyrating'] = train['safty_rating']/(train['past_num_of_claims']+1)\n",
    "test['per_saftyrating'] = test['safty_rating']/(test['past_num_of_claims']+1)\n",
    "\n",
    "\n",
    "##### Delete some fraud_mean variables  #####\n",
    "## The best result is to keep fraud_vehicle_color and fraud_state\n",
    "train.drop([\"fraud_gender\", \"fraud_marital_status\", \"fraud_high_education_ind\", \"fraud_address_change_ind\", \n",
    "              \"fraud_living_status\", \"fraud_zip_code\", \"fraud_claim_date\", \"fraud_witness_present_ind\", \n",
    "              \"fraud_policy_report_filed_ind\", \"fraud_accident_site\", \"fraud_channel\", \"fraud_vehicle_category\",\n",
    "           \"fraud_vehicle_color\", \"fraud_state\",\"Unem_rate\"],\n",
    "              axis = 1, inplace = True)\n",
    "test.drop([\"fraud_gender\", \"fraud_marital_status\", \"fraud_high_education_ind\", \"fraud_address_change_ind\", \n",
    "              \"fraud_living_status\", \"fraud_zip_code\", \"fraud_claim_date\", \"fraud_witness_present_ind\", \n",
    "              \"fraud_policy_report_filed_ind\", \"fraud_accident_site\", \"fraud_channel\", \"fraud_vehicle_category\",\n",
    "          \"fraud_vehicle_color\", \"fraud_state\", \"Unem_rate\"],\n",
    "              axis = 1, inplace = True)\n",
    "train = train.filter(regex=\"^(?!state_).*$\")\n",
    "test = test.filter(regex=\"^(?!state_).*$\")\n",
    "\n",
    "train_xgb = train.copy()\n",
    "test_xgb = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read full training data set\n",
    "df_train = pd.read_csv('../data/train_data_clean_5_grouped.csv')\n",
    "gender_dummies = pd.get_dummies(df_train['gender'], \n",
    "                             prefix = 'gender', drop_first = True)\n",
    "df_train = pd.concat([df_train, gender_dummies], axis = 1)\n",
    "df_train.drop([\"gender\"], axis = 1, inplace = True)\n",
    "\n",
    "living_status_dummies = pd.get_dummies(df_train['living_status'], \n",
    "                             prefix = 'living_status', drop_first = True)\n",
    "df_train = pd.concat([df_train, living_status_dummies], axis = 1)\n",
    "df_train.drop([\"living_status\"], axis = 1, inplace = True)\n",
    "\n",
    "state_dummies = pd.get_dummies(df_train['state'], \n",
    "                               prefix = 'state', drop_first = True)\n",
    "df_train = pd.concat([df_train, state_dummies], axis = 1)\n",
    "df_train.drop([\"state\"], axis = 1, inplace = True)\n",
    "\n",
    "df_train = df_train.sample(frac=1, random_state=5)\n",
    "df_train['new_param'] = df_train.apply(lambda col: col['safty_rating']/(col['past_num_of_claims']+1), axis=1)\n",
    "#df_train['prct_payout'] = df_train.apply(lambda col: col['claim_est_payout']/(col['annual_income']), axis=1)\n",
    "#df_train['age_over_safety'] = df_train.apply(lambda col: col['age_of_driver']/(col['safty_rating']+1), axis=1)\n",
    "df_train.set_index('claim_number', inplace=True)\n",
    "df_train.sort_index(inplace=True)\n",
    "df_train.drop(['claim_date','fraud_claim_date','fraud_zip_code',\n",
    "        \"fraud_gender\", \"fraud_marital_status\", 'fraud_accident_site', 'fraud_high_education_ind',\n",
    "         \"fraud_address_change_ind\", \"fraud_living_status\", \"fraud_witness_present_ind\", \n",
    "         \"fraud_policy_report_filed_ind\", \"fraud_channel\", \"fraud_vehicle_category\",\n",
    "         'fraud_vehicle_color', 'fraud_state', 'SP_Index', 'Unem_rate'], axis = 1, inplace = True)\n",
    "df_train = df_train.filter(regex=\"^(?!state_).*$\")\n",
    "df_train = df_train.filter(regex=\"^(?!vehicle_color_).*$\")\n",
    "df_train = df_train.filter(regex=\"^(?!claim_day_).*$\")\n",
    "df_train = df_train.filter(regex=\"^(?!claim_month_).*$\")\n",
    "\n",
    "train_lgb = df_train.copy()\n",
    "\n",
    "\n",
    "# read full testing data set\n",
    "df_test = pd.read_csv('../data/test_data_clean_5_grouped.csv')\n",
    "gender_dummies = pd.get_dummies(df_test['gender'], \n",
    "                             prefix = 'gender', drop_first = True)\n",
    "df_test = pd.concat([df_test, gender_dummies], axis = 1)\n",
    "df_test.drop([\"gender\"], axis = 1, inplace = True)\n",
    "\n",
    "living_status_dummies = pd.get_dummies(df_test['living_status'], \n",
    "                             prefix = 'living_status', drop_first = True)\n",
    "df_test = pd.concat([df_test, living_status_dummies], axis = 1)\n",
    "df_test.drop([\"living_status\"], axis = 1, inplace = True)\n",
    "\n",
    "state_dummies = pd.get_dummies(df_test['state'], \n",
    "                               prefix = 'state', drop_first = True)\n",
    "df_test = pd.concat([df_test, state_dummies], axis = 1)\n",
    "df_test.drop([\"state\"], axis = 1, inplace = True)\n",
    "\n",
    "#df_test = df_test.sample(frac=1, random_state=5)\n",
    "df_test['new_param'] = df_test.apply(lambda col: col['safty_rating']/(col['past_num_of_claims']+1), axis=1)\n",
    "#df_test['prct_payout'] = df_test.apply(lambda col: col['claim_est_payout']/(col['annual_income']), axis=1)\n",
    "#df_test['age_over_safety'] = df_test.apply(lambda col: col['age_of_driver']/(col['safty_rating']+1), axis=1)\n",
    "\n",
    "df_test.set_index('claim_number', inplace=True)\n",
    "df_test.sort_index(inplace=True)\n",
    "df_test.drop(['claim_date','fraud_claim_date','fraud_zip_code',\n",
    "        \"fraud_gender\", \"fraud_marital_status\", 'fraud_accident_site', 'fraud_high_education_ind',\n",
    "         \"fraud_address_change_ind\", \"fraud_living_status\", \"fraud_witness_present_ind\", \n",
    "         \"fraud_policy_report_filed_ind\", \"fraud_channel\", \"fraud_vehicle_category\",\n",
    "         'fraud_vehicle_color', 'fraud_state', 'SP_Index', 'Unem_rate'], axis = 1, inplace = True)\n",
    "df_test = df_test.filter(regex=\"^(?!state_).*$\")\n",
    "df_test = df_test.filter(regex=\"^(?!vehicle_color_).*$\")\n",
    "df_test = df_test.filter(regex=\"^(?!claim_day_).*$\")\n",
    "df_test = df_test.filter(regex=\"^(?!claim_month_).*$\")\n",
    "\n",
    "test_lgb = df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After discussion, I found that if I directly use the whole training set to choose threshold, then it would overfit. Since I used the prediction on the training dataset based on the fit result on the training dataset. So, I need to use CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CV to get the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first stage, I used several threads to calculate the cost in the threshold range of (0.01, 1). And I found that I could restrict the range to (0.25, 0.5). The optimal result is 0.36, then I further restrict the range to (0.35, 0.37) with smaller step and used several threads to calculate the optimal. The optimal result is 0.364, then I further restrict the range to (0.363, 0.365). The optimal result is 0.36384 and 0.36432. The results of these two are not significantly different, so I just use 0.364."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the cost for misclassification\n",
    "cost_dict = {0: 0, 1: 1, -1: 5}\n",
    "\n",
    "### Set the seed list for splitting dataset\n",
    "seed_list = [100, 150, 200, 250, 300, 350]\n",
    "\n",
    "### Set the parameters of XGBoost and LightGBM\n",
    "clf = xgb.XGBClassifier(max_depth=3,\n",
    "            learning_rate=0.06,\n",
    "            n_estimators=180,\n",
    "            silent=True,\n",
    "            objective='binary:logistic',\n",
    "            gamma=0.35,\n",
    "            min_child_weight=5,\n",
    "            max_delta_step=0,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.785,\n",
    "            colsample_bylevel=1,\n",
    "            reg_alpha=0.01,\n",
    "            reg_lambda=1,\n",
    "            scale_pos_weight=1,\n",
    "            seed=1440,\n",
    "            missing=None)\n",
    "\n",
    "lgbm_params = {'boosting_type':'gbdt',  'objective':'binary', 'num_boost_round':800,\n",
    "               'feature_fraction': .321, 'bagging_fraction':0.50, 'min_child_samples':100,  \n",
    "               'min_child_weigh':35, 'max_depth':3, 'num_leaves':2, 'learing_rate':0.15,\n",
    "               'reg_alpha':5, 'reg_lambda': 1.1, 'metric':'auc', 'max_bin': 52,\n",
    "               'colsample_bytree': 0.9, 'subsample': 0.8, 'is_unbalance': 'true'\n",
    "}\n",
    "\n",
    "cost_list = []\n",
    "thre_list = [0.364]  ## to try diffrent range, just modify this code\n",
    "for threshold in thre_list:\n",
    "    cost = []\n",
    "    for seed in seed_list:\n",
    "        ## generate row indexes\n",
    "        random.seed(seed)\n",
    "        rindex =  np.array(sample(range(len(train_xgb)), round(0.7 * len(train_xgb))))\n",
    "\n",
    "        ## Split train dataset into training and validation parts\n",
    "        ## train_xgb and test_xgb are for XGBoost, train_lgb and test_lgb are for LightGBM\n",
    "\n",
    "        training_xgb = train_xgb.iloc[rindex, :]\n",
    "        validation_xgb = train_xgb.drop(train_xgb.index[rindex])\n",
    "\n",
    "        training_lgb = train_lgb.iloc[rindex, :]\n",
    "        validation_lgb = train_lgb.drop(train_lgb.index[rindex])\n",
    "\n",
    "\n",
    "        ### XGBoost\n",
    "        y_training_xgb = training_xgb[\"fraud\"]\n",
    "        X_training_xgb = training_xgb.drop(\"fraud\", 1)\n",
    "        y_validation_xgb = validation_xgb[\"fraud\"]\n",
    "        X_validation_xgb = validation_xgb.drop(\"fraud\", 1)\n",
    "\n",
    "        clf.fit(X_training_xgb, y_training_xgb)\n",
    "        y_validation_prob_xgb = clf.predict_proba(X_validation_xgb)[:,1]\n",
    "\n",
    "\n",
    "        ### LightGBM\n",
    "        y_training_lgb = training_lgb[\"fraud\"]\n",
    "        X_training_lgb = training_lgb.drop(\"fraud\", 1)\n",
    "        y_validation_lgb = validation_lgb[\"fraud\"]\n",
    "        X_validation_lgb = validation_lgb.drop(\"fraud\", 1)\n",
    "\n",
    "\n",
    "        lgbm = LGBMClassifier(**lgbm_params)\n",
    "        lgbm.fit(X_training_lgb.values, y_training_lgb.values)\n",
    "        y_validation_prob_lgb = lgbm.predict_proba(X_validation_lgb.values)[:,1]\n",
    "\n",
    "        ### Combine the result of two models\n",
    "        validation_prob = 0.4 * y_validation_prob_xgb + 0.6 * y_validation_prob_lgb\n",
    "\n",
    "        ### Calculate the cost\n",
    "        validation_pred = (validation_prob > threshold)*1  # a trick to transform boolean into int type\n",
    "        cost.append(sum([cost_dict[i] for i in (validation_pred - y_validation_xgb)]))\n",
    "        \n",
    "    cost_list.append(mean(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.364\n",
      "2923.5\n"
     ]
    }
   ],
   "source": [
    "min_index = cost_list.index(min(cost_list))\n",
    "print(thre_list[min_index])\n",
    "print(cost_list[min_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fraud prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above result, choose threshold = 0.364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6499.554876354543"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Predict on the test dataset\n",
    "cost_dict = {0: 0, 1: 1, -1: 5}\n",
    "test_pred = pd.read_csv('../data/predictions/combined_predictions.csv')\n",
    "test_pred['fraud'] = (test_pred['fraud'] > 0.364)*1\n",
    "test_pred = test_pred.set_index('claim_number')\n",
    "test_pred.to_csv('../data/predictions/fraud_classification.csv')\n",
    "\n",
    "### Estimate the cost on the test dataset\n",
    "cost_list[min_index] * len(test_xgb) / (0.3 * len(train_xgb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
