{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook cleans the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "import csv\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "train_data = pd.read_csv(\"../data/raw_train_data.csv\", dtype={\"zip_code\" : object})\n",
    "test_data = pd.read_csv(\"../data/raw_test_data.csv\", dtype={\"zip_code\" : object})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cope with unrealistic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop fraud = -1. Set annual_income = -1 and age > 100 to be missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fraud\n",
    "train_data = train_data[train_data.fraud != -1]\n",
    "\n",
    "## annual_income\n",
    "train_data.loc[train_data.annual_income==-1, 'annual_income'] = np.nan\n",
    "test_data.loc[test_data.annual_income==-1, 'annual_income'] = np.nan\n",
    "\n",
    "## age_of_driver\n",
    "train_data.loc[train_data.age_of_driver>100, 'age_of_driver'] = np.nan\n",
    "test_data.loc[test_data.age_of_driver>100, 'age_of_driver'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set the claim number as the index we are working from, and set marital status, higher education index, address change index, zip code, wintness indicator, policy report filed, and fraud to categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set claim_number as index:\n",
    "train_data = train_data.set_index('claim_number')\n",
    "test_data = test_data.set_index('claim_number')\n",
    "\n",
    "train_data[\"marital_status\"] = pd.Categorical(train_data[\"marital_status\"])\n",
    "train_data[\"high_education_ind\"] = pd.Categorical(train_data[\"high_education_ind\"])\n",
    "train_data[\"address_change_ind\"] = pd.Categorical(train_data[\"address_change_ind\"])\n",
    "train_data[\"zip_code\"] = pd.Categorical(train_data[\"zip_code\"])\n",
    "train_data[\"witness_present_ind\"] = pd.Categorical(train_data[\"witness_present_ind\"])\n",
    "train_data[\"policy_report_filed_ind\"] = pd.Categorical(train_data[\"policy_report_filed_ind\"])\n",
    "train_data[\"fraud\"] = pd.Categorical(train_data[\"fraud\"])\n",
    "\n",
    "test_data[\"marital_status\"] = pd.Categorical(test_data[\"marital_status\"])\n",
    "test_data[\"high_education_ind\"] = pd.Categorical(test_data[\"high_education_ind\"])\n",
    "test_data[\"address_change_ind\"] = pd.Categorical(test_data[\"address_change_ind\"])\n",
    "test_data[\"zip_code\"] = pd.Categorical(test_data[\"zip_code\"])\n",
    "test_data[\"witness_present_ind\"] = pd.Categorical(test_data[\"witness_present_ind\"])\n",
    "test_data[\"policy_report_filed_ind\"] = pd.Categorical(test_data[\"policy_report_filed_ind\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break the claim dates into year, month, and day of month as separate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_dict = {\n",
    "    1 : 'January',\n",
    "    2 : 'February',\n",
    "    3 : 'March',\n",
    "    4 : 'April',\n",
    "    5 : 'May',\n",
    "    6 : 'June',\n",
    "    7 : 'July',\n",
    "    8 : 'August',\n",
    "    9 : 'September',\n",
    "    10 : 'October',\n",
    "    11 : 'November',\n",
    "    12 : 'December'\n",
    "}\n",
    "### On the train data\n",
    "train_data[\"claim_month\"] = train_data['claim_date'].apply(\n",
    "    lambda x: month_dict[int(re.search(\"^(\\d+)/\", x).group(1))]\n",
    ")\n",
    "train_data[\"claim_day\"] = train_data['claim_date'].apply(\n",
    "    lambda x: int(re.search(\"/(\\d+)/\", x).group(1))\n",
    ")\n",
    "train_data[\"claim_year\"] = train_data['claim_date'].apply(\n",
    "    lambda x: int(re.search(\"/(\\d+)$\", x).group(1))\n",
    ")\n",
    "\n",
    "\n",
    "### On the test data\n",
    "test_data[\"claim_month\"] = test_data['claim_date'].apply(\n",
    "    lambda x: month_dict[int(re.search(\"^(\\d+)/\", x).group(1))]\n",
    ")\n",
    "test_data[\"claim_day\"] = test_data['claim_date'].apply(\n",
    "    lambda x: int(re.search(\"/(\\d+)/\", x).group(1))\n",
    ")\n",
    "test_data[\"claim_year\"] = test_data['claim_date'].apply(\n",
    "    lambda x: int(re.search(\"/(\\d+)$\", x).group(1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new features: Lat/Lon/State, economic indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloaded zip code data from https://www.unitedstateszipcodes.org/zip-code-database/. And use zip code to get the latitude, longitude and state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/zip_code_database.csv\", newline='') as csvfile:\n",
    "    csv_reader = csv.DictReader(csvfile, delimiter=',')\n",
    "    zip_to_lat = {}\n",
    "    zip_to_lon = {}\n",
    "    zip_to_state = {}\n",
    "    for zip_data in csv_reader:\n",
    "        zip_to_lat[zip_data['zip']] = float(zip_data['latitude'])\n",
    "        zip_to_lon[zip_data['zip']] = float(zip_data['longitude'])\n",
    "        zip_to_state[zip_data['zip']] = zip_data['state']\n",
    "        \n",
    "### assuming the '0' zip code is NaN (no such thing as a zip code of 0)   \n",
    "zip_to_lat[np.nan] = np.nan\n",
    "zip_to_lon[np.nan] = np.nan\n",
    "zip_to_state[np.nan] = np.nan\n",
    "\n",
    "zip_to_lat['0'] = np.nan\n",
    "zip_to_lon['0'] = np.nan\n",
    "zip_to_state['0'] = np.nan\n",
    "\n",
    "### transform zip code to latitude, longitude, and state\n",
    "latitude_train = train_data['zip_code'].apply(\n",
    "    lambda x: zip_to_lat[x]\n",
    ")\n",
    "longitude_train = train_data['zip_code'].apply(\n",
    "    lambda x: zip_to_lon[x]\n",
    ")\n",
    "state_train = train_data['zip_code'].apply(\n",
    "    lambda x: zip_to_state[x]\n",
    ")\n",
    "latitude_train.name = 'latitude'\n",
    "longitude_train.name = 'longitude'\n",
    "state_train.name = 'state'\n",
    "\n",
    "latitude_test = test_data['zip_code'].apply(\n",
    "    lambda x: zip_to_lat[x]\n",
    ")\n",
    "longitude_test = test_data['zip_code'].apply(\n",
    "    lambda x: zip_to_lon[x]\n",
    ")\n",
    "state_test = test_data['zip_code'].apply(\n",
    "    lambda x: zip_to_state[x]\n",
    ")\n",
    "latitude_test.name = 'latitude'\n",
    "longitude_test.name = 'longitude'\n",
    "state_test.name = 'state'\n",
    "\n",
    "\n",
    "### Add these new features to the data frame\n",
    "train_data = pd.concat([train_data, latitude_train], axis=1)\n",
    "train_data = pd.concat([train_data, longitude_train], axis=1)\n",
    "train_data = pd.concat([train_data, state_train], axis=1)\n",
    "train_data[\"state\"] = pd.Categorical(train_data[\"state\"])\n",
    "\n",
    "test_data = pd.concat([test_data, latitude_test], axis=1)\n",
    "test_data = pd.concat([test_data, longitude_test], axis=1)\n",
    "test_data = pd.concat([test_data, state_test], axis=1)\n",
    "test_data[\"state\"] = pd.Categorical(test_data[\"state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load economic data and put into dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unemp_data = pd.read_pickle('../data/unemployment_data/unemp_data.pkl')\n",
    "\n",
    "# For train data\n",
    "claim_month = train_data['claim_month'].values\n",
    "claim_state = train_data['state'].values\n",
    "claim_year = train_data['claim_year'].values\n",
    "\n",
    "unemp_rate = []\n",
    "for x in zip(claim_month, claim_state, claim_year): \n",
    "    unemp_rate_lookup = unemp_data[(unemp_data['month']==x[0])\n",
    "                                   & (unemp_data['state']==x[1])\n",
    "                                   & (unemp_data['year']==x[2])]['unemp_rate'].values\n",
    "    if not unemp_rate_lookup:\n",
    "        unemp_rate_lookup = [np.nan]\n",
    "    unemp_rate.append(unemp_rate_lookup[0])\n",
    "train_data['unemp_rate'] = unemp_rate\n",
    "\n",
    "\n",
    "# For test data\n",
    "claim_month = test_data['claim_month'].values\n",
    "claim_state = test_data['state'].values\n",
    "claim_year = test_data['claim_year'].values\n",
    "\n",
    "unemp_rate = []\n",
    "for x in zip(claim_month, claim_state, claim_year): \n",
    "    unemp_rate_lookup = unemp_data[(unemp_data['month']==x[0])\n",
    "                                   & (unemp_data['state']==x[1])\n",
    "                                   & (unemp_data['year']==x[2])]['unemp_rate'].values\n",
    "    if not unemp_rate_lookup:\n",
    "        unemp_rate_lookup = [np.nan]\n",
    "    unemp_rate.append(unemp_rate_lookup[0])\n",
    "test_data['unemp_rate'] = unemp_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation of Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use mode and mean to impute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# mean of age_of_driver\n",
    "age_of_driver_mean = train_data.age_of_driver.mean()\n",
    "train_data['age_of_driver'].fillna(age_of_driver_mean, inplace=True)\n",
    "test_data['age_of_driver'].fillna(age_of_driver_mean, inplace=True)\n",
    "\n",
    "# mode of marital_status\n",
    "marital_status_mode = train_data.marital_status.mode().values[0]\n",
    "train_data['marital_status'].fillna(marital_status_mode, inplace=True)\n",
    "test_data['marital_status'].fillna(marital_status_mode, inplace=True)\n",
    "\n",
    "# average of annual_income\n",
    "annual_income_mean = train_data.annual_income.mean()\n",
    "train_data['annual_income'].fillna(annual_income_mean, inplace=True)\n",
    "test_data['annual_income'].fillna(annual_income_mean, inplace=True)\n",
    "\n",
    "# mode of witness_present_ind\n",
    "witness_present_mode = train_data.witness_present_ind.mode().values[0]\n",
    "train_data['witness_present_ind'].fillna(witness_present_mode, inplace=True)\n",
    "test_data['witness_present_ind'].fillna(witness_present_mode, inplace=True)\n",
    "\n",
    "# mean of claim_est_payout\n",
    "claim_est_payout_mean = train_data.claim_est_payout.mean()\n",
    "train_data['claim_est_payout'].fillna(claim_est_payout_mean, inplace=True)\n",
    "test_data['claim_est_payout'].fillna(claim_est_payout_mean, inplace=True)\n",
    "\n",
    "# mean of age_of_vehicle\n",
    "age_of_vehicle_mean = train_data.age_of_vehicle.mean()\n",
    "train_data['age_of_vehicle'].fillna(age_of_vehicle_mean, inplace=True)\n",
    "test_data['age_of_vehicle'].fillna(age_of_vehicle_mean, inplace=True)\n",
    "\n",
    "# mean latitude\n",
    "latitude_mean = train_data.latitude.mean()\n",
    "train_data['latitude'].fillna(latitude_mean, inplace=True)\n",
    "test_data['latitude'].fillna(latitude_mean, inplace=True)\n",
    "\n",
    "# mean longitude\n",
    "longitude_mean = train_data.longitude.mean()\n",
    "train_data['longitude'].fillna(longitude_mean, inplace=True)\n",
    "test_data['longitude'].fillna(longitude_mean, inplace=True)\n",
    "\n",
    "# mode of state\n",
    "state_mode = train_data.state.mode().values[0]\n",
    "train_data['state'].fillna(state_mode, inplace=True)\n",
    "test_data['state'].fillna(state_mode, inplace=True)\n",
    "\n",
    "# mean of unemp_rate\n",
    "unemp_rate_mean = train_data.unemp_rate.mean()\n",
    "train_data['unemp_rate'].fillna(unemp_rate_mean, inplace=True)\n",
    "test_data['unemp_rate'].fillna(unemp_rate_mean, inplace=True)\n",
    "\n",
    "# print the list of missing columns\n",
    "print(list(itertools.compress(list(train_data), list(train_data.isna().any()))))\n",
    "print(list(itertools.compress(list(test_data), list(test_data.isna().any()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding of Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we've replaced all na values in testing and training.  The next thing I want to do is encode categorical variables that have more than two categories using one hot encoding.  I first check that the same number of categories exist in both the training and testing data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "###encoding for TRAIN data set\n",
    "\n",
    "# one-hot encoding for day of week\n",
    "day_dummies = pd.get_dummies(train_data['claim_day_of_week'], \n",
    "                             prefix='claim_day', drop_first=True)\n",
    "train_data = pd.concat([train_data, day_dummies], axis=1)\n",
    "train_data.drop([\"claim_day_of_week\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for site of accident\n",
    "accident_dummies = pd.get_dummies(train_data['accident_site'], \n",
    "                                  prefix='accident_site', drop_first=True)\n",
    "train_data = pd.concat([train_data, accident_dummies], axis=1)\n",
    "train_data.drop([\"accident_site\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for channel\n",
    "channel_dummies = pd.get_dummies(train_data['channel'], \n",
    "                                 prefix='channel', drop_first=True)\n",
    "train_data = pd.concat([train_data, channel_dummies], axis=1)\n",
    "train_data.drop([\"channel\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for vehicle category\n",
    "vehicle_cat_dummies = pd.get_dummies(train_data['vehicle_category'], \n",
    "                                 prefix='vehicle_category', drop_first=True)\n",
    "train_data = pd.concat([train_data, vehicle_cat_dummies], axis=1)\n",
    "train_data.drop([\"vehicle_category\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for vehicle color\n",
    "vehicle_color_dummies = pd.get_dummies(train_data['vehicle_color'], \n",
    "                                 prefix='vehicle_color', drop_first=True)\n",
    "train_data = pd.concat([train_data, vehicle_color_dummies], axis=1)\n",
    "train_data.drop([\"vehicle_color\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for claim month\n",
    "vehicle_color_dummies = pd.get_dummies(train_data['claim_month'], \n",
    "                                 prefix='claim_month', drop_first=True)\n",
    "train_data = pd.concat([train_data, vehicle_color_dummies], axis=1)\n",
    "train_data.drop([\"claim_month\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for state\n",
    "state_dummies = pd.get_dummies(train_data['state'],\n",
    "                               prefix='state', drop_first=True)\n",
    "train_data = pd.concat([train_data, state_dummies], axis=1)\n",
    "train_data.drop([\"state\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "### encoding for TEST data set\n",
    "\n",
    "# one-hot encoding for day of week\n",
    "day_dummies = pd.get_dummies(test_data['claim_day_of_week'], \n",
    "                             prefix='claim_day', drop_first=True)\n",
    "test_data = pd.concat([test_data, day_dummies], axis=1)\n",
    "test_data.drop([\"claim_day_of_week\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for site of accident\n",
    "accident_dummies = pd.get_dummies(test_data['accident_site'], \n",
    "                                  prefix='accident_site', drop_first=True)\n",
    "test_data = pd.concat([test_data, accident_dummies], axis=1)\n",
    "test_data.drop([\"accident_site\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for channel\n",
    "channel_dummies = pd.get_dummies(test_data['channel'], \n",
    "                                 prefix='channel', drop_first=True)\n",
    "test_data = pd.concat([test_data, channel_dummies], axis=1)\n",
    "test_data.drop([\"channel\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for vehicle category\n",
    "vehicle_cat_dummies = pd.get_dummies(test_data['vehicle_category'], \n",
    "                                 prefix='vehicle_category', drop_first=True)\n",
    "test_data = pd.concat([test_data, vehicle_cat_dummies], axis=1)\n",
    "test_data.drop([\"vehicle_category\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for vehicle color\n",
    "vehicle_color_dummies = pd.get_dummies(test_data['vehicle_color'], \n",
    "                                 prefix='vehicle_color', drop_first=True)\n",
    "test_data = pd.concat([test_data, vehicle_color_dummies], axis=1)\n",
    "test_data.drop([\"vehicle_color\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for claim month\n",
    "vehicle_color_dummies = pd.get_dummies(test_data['claim_month'], \n",
    "                                 prefix='claim_month', drop_first=True)\n",
    "test_data = pd.concat([test_data, vehicle_color_dummies], axis=1)\n",
    "test_data.drop([\"claim_month\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for state\n",
    "state_dummies = pd.get_dummies(test_data['state'],\n",
    "                               prefix='state', drop_first=True)\n",
    "test_data = pd.concat([test_data, state_dummies], axis=1)\n",
    "test_data.drop([\"state\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### clean up variable names by making them all lowercase with underscore separators.\n",
    "train_data.columns = map(\n",
    "    lambda s: s.lower().replace(' ', '_'), \n",
    "    train_data.columns\n",
    ")\n",
    "\n",
    "test_data.columns = map(\n",
    "    lambda s: s.lower().replace(' ', '_'), \n",
    "    test_data.columns\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add interest rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Int=pd.read_csv('data/Interest_rate.csv')  ## the dataset is a little bit different from the original one. Check later.\n",
    "\n",
    "month_dict = {\n",
    "  1 : 'M01', 2 : 'M02', 3 : 'M03', 4 : 'M04', 5 : 'M05',6 : 'M06',7 : 'M07',8 : 'M08',9 : 'M09',10 : 'M10', 11 : 'M11',12 : 'M12'\n",
    "}\n",
    "\n",
    "for key in month_dict:\n",
    "    train_data.loc[(train_data['claim_month']==month_dict[key])&(train_data['claim_year']==2015),'Interest_rate']=Int.loc[key-1,'Interest_rate']\n",
    "    train_data.loc[(train_data['claim_month']==month_dict[key])&(train_data['claim_year']==2016),'Interest_rate']=Int.loc[key+11,'Interest_rate']\n",
    "    test_data.loc[(test_data['claim_month']==month_dict[key])&(test_data['claim_year']==2015),'Interest_rate']=Int.loc[key-1,'Interest_rate']\n",
    "    test_data.loc[(test_data['claim_month']==month_dict[key])&(test_data['claim_year']==2016),'Interest_rate']=Int.loc[key+11,'Interest_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.to_csv('../data/train_data_clean.csv', header=True)\n",
    "test_data.to_csv('../data/test_data_clean.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the loss of some original code and data, some details are not included in this notebook. And the final data cleaning file is \"../data/train_data_clean3.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add grouped-by means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vehicle_color</th>\n",
       "      <th>fraud_vehicle_color</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vehicle_color</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>black</th>\n",
       "      <td>black</td>\n",
       "      <td>0.162977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>blue</td>\n",
       "      <td>0.153454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gray</th>\n",
       "      <td>gray</td>\n",
       "      <td>0.154753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>other</td>\n",
       "      <td>0.154274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>red</th>\n",
       "      <td>red</td>\n",
       "      <td>0.157505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>silver</th>\n",
       "      <td>silver</td>\n",
       "      <td>0.159221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>white</td>\n",
       "      <td>0.151644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              vehicle_color  fraud_vehicle_color\n",
       "vehicle_color                                   \n",
       "black                 black             0.162977\n",
       "blue                   blue             0.153454\n",
       "gray                   gray             0.154753\n",
       "other                 other             0.154274\n",
       "red                     red             0.157505\n",
       "silver               silver             0.159221\n",
       "white                 white             0.151644"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## read data\n",
    "train = pd.read_csv(\"../data/train_data_clean3.csv\")\n",
    "test = pd.read_csv(\"../data/test_data_clean3.csv\")\n",
    "raw_train = pd.read_csv('../data/raw_train_data.csv')\n",
    "raw_test = pd.read_csv('../data/raw_test_data.csv')\n",
    "\n",
    "\n",
    "## add Unemployment data\n",
    "df1 = pd.read_csv(\"../data/train_data_clean2.csv\")\n",
    "train['Unem_rate'] = df1['Unem_rate']\n",
    "df2 = pd.read_csv('../data/test_data_clean2.csv')\n",
    "test['Unem_rate'] = df2['Unem_rate']\n",
    "\n",
    "## add interest data\n",
    "train_interest = pd.read_csv(\"../data/train_data_interest.csv\")\n",
    "train['Interest_rate'] = train_interest['Interest_rate']\n",
    "test_interest = pd.read_csv(\"../data/test_data_interest.csv\")\n",
    "test['Interest_rate'] = test_interest['Interest_rate']\n",
    "\n",
    "\n",
    "## gender\n",
    "grouped_gender = train[\"fraud\"].groupby(train['gender'])\n",
    "grouped_gender_mean = grouped_gender.mean().to_frame()\n",
    "grouped_gender_mean['gender']=grouped_gender_mean.index\n",
    "grouped_gender_mean['fraud_gender'] = grouped_gender_mean['fraud']\n",
    "grouped_gender_mean.drop('fraud', axis = 1, inplace = True)\n",
    "train = pd.merge(train, grouped_gender_mean, on = \"gender\", how = \"left\")\n",
    "test = pd.merge(test, grouped_gender_mean, on = \"gender\", how = \"left\")\n",
    "grouped_gender_mean\n",
    "\n",
    "## marital_status\n",
    "grouped_marital_status = train[\"fraud\"].groupby(train['marital_status'])\n",
    "grouped_marital_status_mean = grouped_marital_status.mean().to_frame()\n",
    "grouped_marital_status_mean['marital_status']=grouped_marital_status_mean.index\n",
    "grouped_marital_status_mean['fraud_marital_status'] = grouped_marital_status_mean['fraud']\n",
    "grouped_marital_status_mean.drop('fraud', axis = 1, inplace = True)\n",
    "train = pd.merge(train, grouped_marital_status_mean, on = \"marital_status\", how = \"left\")\n",
    "test = pd.merge(test, grouped_marital_status_mean, on = \"marital_status\", how = \"left\")\n",
    "grouped_marital_status_mean\n",
    "\n",
    "## high_education_ind\n",
    "grouped_high_education_ind = train[\"fraud\"].groupby(train['high_education_ind'])\n",
    "grouped_high_education_ind_mean = grouped_high_education_ind.mean().to_frame()\n",
    "grouped_high_education_ind_mean['high_education_ind']=grouped_high_education_ind_mean.index\n",
    "grouped_high_education_ind_mean['fraud_high_education_ind'] = grouped_high_education_ind_mean['fraud']\n",
    "grouped_high_education_ind_mean.drop('fraud', axis = 1, inplace = True)\n",
    "train = pd.merge(train, grouped_high_education_ind_mean, on = \"high_education_ind\", how = \"left\")\n",
    "test = pd.merge(test, grouped_high_education_ind_mean, on = \"high_education_ind\", how = \"left\")\n",
    "grouped_high_education_ind_mean\n",
    "\n",
    "## address_change_ind\n",
    "grouped_address_change_ind = train[\"fraud\"].groupby(train['address_change_ind'])\n",
    "grouped_address_change_ind_mean = grouped_address_change_ind.mean().to_frame()\n",
    "grouped_address_change_ind_mean['address_change_ind']=grouped_address_change_ind_mean.index\n",
    "grouped_address_change_ind_mean['fraud_address_change_ind'] = grouped_address_change_ind_mean['fraud']\n",
    "grouped_address_change_ind_mean.drop('fraud', axis = 1, inplace = True)\n",
    "train = pd.merge(train, grouped_address_change_ind_mean, on = \"address_change_ind\", how = \"left\")\n",
    "test = pd.merge(test, grouped_address_change_ind_mean, on = \"address_change_ind\", how = \"left\")\n",
    "grouped_address_change_ind_mean\n",
    "\n",
    "## living_status\n",
    "grouped_living_status = train[\"fraud\"].groupby(train['living_status'])\n",
    "grouped_living_status_mean = grouped_living_status.mean().to_frame()\n",
    "grouped_living_status_mean['living_status']=grouped_living_status_mean.index\n",
    "grouped_living_status_mean['fraud_living_status'] = grouped_living_status_mean['fraud']\n",
    "grouped_living_status_mean.drop('fraud', axis = 1, inplace = True)\n",
    "train = pd.merge(train, grouped_living_status_mean, on = \"living_status\", how = \"left\")\n",
    "test = pd.merge(test, grouped_living_status_mean, on = \"living_status\", how = \"left\")\n",
    "grouped_living_status_mean\n",
    "\n",
    "## zip_code\n",
    "grouped_zip_code = train[\"fraud\"].groupby(train['zip_code'])\n",
    "grouped_zip_code_mean = grouped_zip_code.mean().to_frame()\n",
    "grouped_zip_code_mean['zip_code']=grouped_zip_code_mean.index\n",
    "grouped_zip_code_mean['fraud_zip_code'] = grouped_zip_code_mean['fraud']\n",
    "grouped_zip_code_mean.drop('fraud', axis = 1, inplace = True)\n",
    "train = pd.merge(train, grouped_zip_code_mean, on = \"zip_code\", how = \"left\")\n",
    "test = pd.merge(test, grouped_zip_code_mean, on = \"zip_code\", how = \"left\")\n",
    "grouped_zip_code_mean\n",
    "\n",
    "## claim_date\n",
    "grouped_claim_date = train[\"fraud\"].groupby(train['claim_date'])\n",
    "grouped_claim_date_mean = grouped_claim_date.mean().to_frame()\n",
    "grouped_claim_date_mean['claim_date']=grouped_claim_date_mean.index\n",
    "grouped_claim_date_mean['fraud_claim_date'] = grouped_claim_date_mean['fraud']\n",
    "grouped_claim_date_mean.drop('fraud', axis = 1, inplace = True)\n",
    "train = pd.merge(train, grouped_claim_date_mean, on = \"claim_date\", how = \"left\")\n",
    "test = pd.merge(test, grouped_claim_date_mean, on = \"claim_date\", how = \"left\")\n",
    "grouped_claim_date_mean\n",
    "\n",
    "## witness_present_ind\n",
    "grouped_witness_present_ind = train[\"fraud\"].groupby(train['witness_present_ind'])\n",
    "grouped_witness_present_ind_mean = grouped_witness_present_ind.mean().to_frame()\n",
    "grouped_witness_present_ind_mean['witness_present_ind']=grouped_witness_present_ind_mean.index\n",
    "grouped_witness_present_ind_mean['fraud_witness_present_ind'] = grouped_witness_present_ind_mean['fraud']\n",
    "grouped_witness_present_ind_mean.drop('fraud', axis = 1, inplace = True)\n",
    "train = pd.merge(train, grouped_witness_present_ind_mean, on = \"witness_present_ind\", how = \"left\")\n",
    "test = pd.merge(test, grouped_witness_present_ind_mean, on = \"witness_present_ind\", how = \"left\")\n",
    "grouped_witness_present_ind_mean\n",
    "\n",
    "## policy_report_filed_ind\n",
    "grouped_policy_report_filed_ind = train[\"fraud\"].groupby(train['policy_report_filed_ind'])\n",
    "grouped_policy_report_filed_ind_mean = grouped_policy_report_filed_ind.mean().to_frame()\n",
    "grouped_policy_report_filed_ind_mean['policy_report_filed_ind']=grouped_policy_report_filed_ind_mean.index\n",
    "grouped_policy_report_filed_ind_mean['fraud_policy_report_filed_ind'] = grouped_policy_report_filed_ind_mean['fraud']\n",
    "grouped_policy_report_filed_ind_mean.drop('fraud', axis = 1, inplace = True)\n",
    "train = pd.merge(train, grouped_policy_report_filed_ind_mean, on = \"policy_report_filed_ind\", how = \"left\")\n",
    "test = pd.merge(test, grouped_policy_report_filed_ind_mean, on = \"policy_report_filed_ind\", how = \"left\")\n",
    "grouped_policy_report_filed_ind_mean\n",
    "\n",
    "## state\n",
    "grouped_state = train[\"fraud\"].groupby(train['state'])\n",
    "grouped_state_mean = grouped_state.mean().to_frame()\n",
    "grouped_state_mean['state']=grouped_state_mean.index\n",
    "grouped_state_mean['fraud_state'] = grouped_state_mean['fraud']\n",
    "grouped_state_mean.drop('fraud', axis = 1, inplace = True)\n",
    "train = pd.merge(train, grouped_state_mean, on = \"state\", how = \"left\")\n",
    "test = pd.merge(test, grouped_state_mean, on = \"state\", how = \"left\")\n",
    "grouped_state_mean\n",
    "\n",
    "## accident_site\n",
    "grouped_accident_site = raw_train[\"fraud\"].groupby(raw_train['accident_site'])\n",
    "grouped_accident_site_mean = grouped_accident_site.mean().to_frame()\n",
    "grouped_accident_site_mean['accident_site']=grouped_accident_site_mean.index\n",
    "grouped_accident_site_mean['fraud_accident_site'] = grouped_accident_site_mean['fraud']\n",
    "grouped_accident_site_mean.drop('fraud', axis = 1, inplace = True)\n",
    "raw_train = pd.merge(raw_train, grouped_accident_site_mean, on = \"accident_site\", how = \"left\")\n",
    "train['fraud_accident_site'] = raw_train['fraud_accident_site']\n",
    "raw_test = pd.merge(raw_test, grouped_accident_site_mean, on = \"accident_site\", how = \"left\")\n",
    "test['fraud_accident_site'] = raw_test['fraud_accident_site']\n",
    "grouped_accident_site_mean\n",
    "\n",
    "## channel\n",
    "grouped_channel = raw_train[\"fraud\"].groupby(raw_train['channel'])\n",
    "grouped_channel_mean = grouped_channel.mean().to_frame()\n",
    "grouped_channel_mean['channel']=grouped_channel_mean.index\n",
    "grouped_channel_mean['fraud_channel'] = grouped_channel_mean['fraud']\n",
    "grouped_channel_mean.drop('fraud', axis = 1, inplace = True)\n",
    "raw_train = pd.merge(raw_train, grouped_channel_mean, on = \"channel\", how = \"left\")\n",
    "train['fraud_channel'] = raw_train['fraud_channel']\n",
    "raw_test = pd.merge(raw_test, grouped_channel_mean, on = \"channel\", how = \"left\")\n",
    "test['fraud_channel'] = raw_test['fraud_channel']\n",
    "              \n",
    "grouped_channel_mean\n",
    "\n",
    "## vehicle_category\n",
    "grouped_vehicle_category = raw_train[\"fraud\"].groupby(raw_train['vehicle_category'])\n",
    "grouped_vehicle_category_mean = grouped_vehicle_category.mean().to_frame()\n",
    "grouped_vehicle_category_mean['vehicle_category']=grouped_vehicle_category_mean.index\n",
    "grouped_vehicle_category_mean['fraud_vehicle_category'] = grouped_vehicle_category_mean['fraud']\n",
    "grouped_vehicle_category_mean.drop('fraud', axis = 1, inplace = True)\n",
    "raw_train = pd.merge(raw_train, grouped_vehicle_category_mean, on = \"vehicle_category\", how = \"left\")\n",
    "train['fraud_vehicle_category'] = raw_train['fraud_vehicle_category']\n",
    "raw_test = pd.merge(raw_test, grouped_vehicle_category_mean, on = \"vehicle_category\", how = \"left\")\n",
    "test['fraud_vehicle_category'] = raw_test['fraud_vehicle_category']              \n",
    "grouped_vehicle_category_mean\n",
    "\n",
    "## vehicle_color\n",
    "grouped_vehicle_color = raw_train[\"fraud\"].groupby(raw_train['vehicle_color'])\n",
    "grouped_vehicle_color_mean = grouped_vehicle_color.mean().to_frame()\n",
    "grouped_vehicle_color_mean['vehicle_color']=grouped_vehicle_color_mean.index\n",
    "grouped_vehicle_color_mean['fraud_vehicle_color'] = grouped_vehicle_color_mean['fraud']\n",
    "grouped_vehicle_color_mean.drop('fraud', axis = 1, inplace = True)\n",
    "raw_train = pd.merge(raw_train, grouped_vehicle_color_mean, on = \"vehicle_color\", how = \"left\")\n",
    "train['fraud_vehicle_color'] = raw_train['fraud_vehicle_color']\n",
    "raw_test = pd.merge(raw_test, grouped_vehicle_color_mean, on = \"vehicle_color\", how = \"left\")\n",
    "test['fraud_vehicle_color'] = raw_test['fraud_vehicle_color']              \n",
    "grouped_vehicle_color_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing need to note is that the above code will cause problems of overfitting and that is why we delete all of the grouped by means in the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../data/train_data_clean_4_grouped.csv', index = False)\n",
    "test.to_csv('../data/test_data_clean_4_grouped.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
